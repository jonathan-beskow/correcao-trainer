from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
from transformers import AutoTokenizer, AutoModel
import torch
import requests
import os
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente
load_dotenv()
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

app = FastAPI()

print("üîê Verificando chave da API OpenRouter...")
if OPENROUTER_API_KEY:
    print("‚úÖ Chave carregada com sucesso (oculta por seguran√ßa)")
else:
    print("‚ùå Nenhuma chave foi carregada. Verifique o .env e a configura√ß√£o do Docker.")


# Evento de inicializa√ß√£o para testar a chave da API
@app.on_event("startup")
async def verificar_login_openrouter():
    print("üîê Verificando chave da API OpenRouter...")
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "openai/gpt-3.5-turbo",
        "messages": [
            {"role": "user", "content": "Ol√°, voc√™ est√° funcionando?"}
        ]
    }

    try:
        response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload)
        response.raise_for_status()
        print("‚úÖ Conex√£o com a OpenRouter bem-sucedida! API key v√°lida.")
    except Exception as e:
        print(f"‚ùå Erro ao conectar com OpenRouter. Verifique sua chave. Detalhes: {str(e)}")

# Modelo para gerar embeddings (mantido local)
tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModel.from_pretrained("microsoft/codebert-base")

class EmbeddingRequest(BaseModel):
    codigo: str
    tipo: str

@app.post("/embed")
async def gerar_embedding(req: EmbeddingRequest):
    entrada = f"{req.tipo}: {req.codigo}"
    tokens = tokenizer(entrada, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**tokens)
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()
    return {"embedding": embedding}


# Dados para corre√ß√£o com IA (via OpenRouter)
class ExemploCorrecao(BaseModel):
    codigo_original: str
    codigo_corrigido: str

class CorrecaoRequest(BaseModel):
    tipo: str
    codigo_alvo: str
    exemplos: List[ExemploCorrecao]

@app.post("/gerar-correcao")
async def gerar_correcao(req: CorrecaoRequest):
    prompt = f"""
Voc√™ √© um assistente especializado em seguran√ßa de software. Dada uma vulnerabilidade do tipo {req.tipo},
e um exemplo de c√≥digo vulner√°vel com sua respectiva corre√ß√£o, voc√™ deve corrigir um novo c√≥digo que cont√©m a mesma falha.

Exemplo:
C√≥digo vulner√°vel:
{req.exemplos[0].codigo_original}

C√≥digo corrigido:
{req.exemplos[0].codigo_corrigido}

Agora corrija o seguinte c√≥digo:
{req.codigo_alvo}

C√≥digo corrigido:
"""

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "openai/gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": "Voc√™ √© um especialista em corre√ß√£o de c√≥digo seguro."},
            {"role": "user", "content": prompt}
        ]
    }

    try:
        response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload)
        response.raise_for_status()
        content = response.json()["choices"][0]["message"]["content"].strip()

        if not content:
            return {"codigoCorrigido": "A IA n√£o retornou nenhuma sugest√£o. Verifique o prompt ou os exemplos."}

        return {"codigoCorrigido": content}

    except Exception as e:
        return {"codigoCorrigido": None, "erro": str(e)}
